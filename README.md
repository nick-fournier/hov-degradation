# hov-degradation
Serves as the repository for HOV-degradation analysis

## Contents

* [Setup Instructions](#setup-instructions)
* [Usage](#usage)

## Setup Instructions


### Executable


### Developer
If you have not done so already, download the hov-degradation github repository.

```bash
git clone https://github.com/nick-fournier/hov-degradation
cd hov-analysis
```

Next, create the Conda environment and install hov-degradation and its
dependencies within the environment. You can install Conda from
[Anaconda](https://www.anaconda.com/download)

```bash
conda env create -f environment.yml
source activate hov-analysis
python setup.py develop
```

If the setup.py installations fails, install the contents using pip

```bash
pip install -e .
```

## Usage


To run the code, it requires the file path for an input and output data folder to direct the code. I've organized my directory like this, but you can input a custom file paths to any desired input and output paths:
```
HOV degradation main directory
├─ hov_degradation code
│   ├─ preprocess
│   ├─ analysis
│   ├─ reporting
│   └─ static
└─ data
    ├─ input
    │   ├─ 5min_counts
    │   └─ hourly_counts
    └─ output
```
Two main types of data are needed:
* **5-minute daily traffic counts**: used for detection of misconfigured sensors.
* **Hourly traffic counts**: used for the degradation analysis *of* the detected sensors.

The file format is the compressed `.txt.gz` filetype downloaded from PeMS, no extraction necessary. 

For 5-minute data I downloaded 7 consecutive days of 5-minute traffic counts. I found more than 7 days of data and the combined dataset becomes a bit too large. It's technically possible to use more than 7 days, but the benefit is likely negligible.

For both of data sources, a meta-data file for sensors is also required to be placed in each folder. Note that the meta-data file could be the same for both 5-minute and hourly counts, but the code requires a separate meta-data file for each data type. This is for consistency in case the study period varies so that the meta-data file is different. 

The code is organized into three overall steps:
1. **Preprocessing**
2. **Analysis** 
3. **Reporting**


### 1. Pre-processing
In this step the "raw" unfiltered daily 5-minute traffic count data are processed by first removing sensors that do not meet the minimum data requirements and then performing feature extraction. The features are average nighttime flow/occupancy and the K-S statistic) of the flow profiles. Intermediate output is saved in a new folder `output/processed` which will contain four new files generated by the code.
```
output/processed/
    ├─ processed_i210_test_<start date>_to_<end date>.csv
    ├─ processed_i210_train_<start date>_to_<end date>.csv
    ├─ processed_D<district #>_<start date>_to_<end date>.csv
    └─ neighbors_D<district #>_<start date>_to_<end date>.json
```
The three "processed" files contain the extracted feature data from the sensor data, two of the files are for testing and training of the machine learning algorithms using the validated I-210 data and the third is for the district-wide detection using the trained model. The "neigbors" file is a json file containing the data-dictionary relating each HOV sensor to its nearest upstream/downstream HOV sensor and its adjacent mainline sensors.

 
### 2. Analysis
In this step the processed data generated in the previous step are then analyzed in two sub-steps. 
   1. *Training & testing of I-210 data:* First, all machine learning methods are trained and tested on the I-210 data to obtain the tuned hyperparameters for each method, where applicable, and to determine each machine learning method's prediction accuracy, which are saved to:
```
output/results/
    ├─ analysis_scores_<start date>_to_<end date>.csv
    └─ analysis_hyperparameters_<start date>_to_<end date>.json
```
   2. *District-wide detection:* The best models are then selected from the scoring (Random Forest and Local Outlier Factor used as default for now) and run over the entire district-wide sensor data using the tuned hyperparameters. This second step will then generate district-wide results:
```
output/results/
    ├─ analysis_detections_table_D<district #>_<start date>_to_<end date>.csv
    ├─ analysis_misconfigs_meta_table_D<district #>_<start date>_to_<end date>.csv
    ├─ analysis_misconfigs_ids_D<district #>_<start date>_to_<end date>.csv
    └─ analysis_neighbors_D<district #>_<start date>_to_<end date>.json
```

### 3. Reporting
df


### 4. Degradation
df
